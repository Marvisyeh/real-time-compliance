# ğŸ“¡ Real-Time Event Monitoring Platform

A real-time data pipeline that ingests system logs, metrics, and transaction events through Kafka, performs cleaning and anomaly detection, stores cleaned data into PostgreSQL, triggers Slack alerts on anomalies, and visualizes everything in a React dashboard.

This project demonstrates an end-to-end **streaming data engineering architecture** with **real-time ETL**, **anomaly detection**, **cloud-like storage**, **API services**, and **interactive visualizations**.

---

## ğŸš€ Features

- **Real-time streaming pipeline** with Kafka
- **Producers** generate mock logs, metrics, and transaction events
- **Consumer A** writes raw events to S3 (raw data lake)
- **Consumer B** performs:

  - JSON parsing & data cleaning
  - Rule-based anomaly detection
  - Writes cleaned data into PostgreSQL
  - Sends Slack alerts for anomalies

- **FastAPI** backend to expose metrics, logs, anomalies, and statistics
<!-- - **React Dashboard** to visualize real-time trends and anomalies -->

---

## ğŸ— Architecture Overview

```
Producers (Python)
     â”‚
     â–¼
 Kafka Topics (logs / metrics / transactions)
     â”‚
     â”œâ”€â”€ Consumer A â†’ S3 Raw Storage
     â”‚
     â””â”€â”€ Consumer B â†’ Cleaning â†’ Anomaly Detection â†’ PostgreSQL â†’ Slack Alert
                                               â”‚
                                               â–¼
                                          FastAPI API
                                               â”‚
                                               â–¼
                                       React Dashboard(planing)
```

---

## ğŸ”§ Tech Stack

### **Backend / Data Pipeline**

- Python
- Kafka (Producers + Consumers)
- S3 (raw data storage)
- PostgreSQL (cleaned events + anomalies)
- FastAPI
- Slack Webhook for alerting

<!-- ### **Frontend** (Coming Soon)

- React + Charting Library

---

## ğŸ“ Project Structure (Coming Soon)

```
/src
  /producers
  /consumer_a_raw_sink
  /consumer_b_etl_anomaly
  /api
  /dashboard
/infra
  /docker
  /config
/docs
  architecture-diagram.png
  anomaly-pipeline.png
README.md
```

---

## ğŸ”¥ Example Event Format

**Log Event**

```json
{
  "timestamp": "2025-01-01T10:20:30Z",
  "service": "auth-service",
  "level": "ERROR",
  "message": "User login failed",
  "user_id": 123
}
```

**Metric Event**

```json
{
  "timestamp": "2025-01-01T10:20:32Z",
  "service": "inventory-api",
  "cpu": 88.2,
  "latency_ms": 420
}
```

---

## âš ï¸ Anomaly Detection Rules (Coming Soon)

- **Logs**: > 20 ERROR logs within 1 minute
- **Metrics**: CPU > 85% or Latency > 400ms
- **Transactions**: Amount > 10,000

Detected anomalies will be stored in PostgreSQL and notified via Slack.

---

## ğŸ§ª How to Run (Coming Soon)

Documentation for running Kafka, consumers, API, and dashboard will be added as implementation progresses.

---

## ğŸ“Œ Project Goals

- Build a production-style real-time data platform
- Demonstrate data pipeline & streaming engineering skills
- Show an end-to-end system: ingestion â†’ processing â†’ storage â†’ API â†’ visualization
- Serve as a portfolio project for Data / AI Engineering roles -->
